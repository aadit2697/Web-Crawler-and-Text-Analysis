{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a number in multiples of 20 : 40\n",
      "this is page 0\n",
      "October 16, 2019\n",
      "/cmp/Oyo/reviews?start=20\n",
      "October 27, 2019\n",
      "/cmp/Oyo/reviews?start=20\n",
      "October 17, 2019\n",
      "/cmp/Oyo/reviews?start=20\n",
      "October 14, 2019\n",
      "/cmp/Oyo/reviews?start=20\n",
      "October 14, 2019\n",
      "/cmp/Oyo/reviews?start=20\n",
      "October 10, 2019\n",
      "/cmp/Oyo/reviews?start=20\n",
      "October 9, 2019\n",
      "/cmp/Oyo/reviews?start=20\n",
      "October 6, 2019\n",
      "/cmp/Oyo/reviews?start=20\n",
      "October 4, 2019\n",
      "/cmp/Oyo/reviews?start=20\n",
      "October 3, 2019\n",
      "/cmp/Oyo/reviews?start=20\n",
      "October 3, 2019\n",
      "/cmp/Oyo/reviews?start=20\n",
      "October 3, 2019\n",
      "/cmp/Oyo/reviews?start=20\n",
      "September 30, 2019\n",
      "/cmp/Oyo/reviews?start=20\n",
      "September 29, 2019\n",
      "/cmp/Oyo/reviews?start=20\n",
      "September 24, 2019\n",
      "/cmp/Oyo/reviews?start=20\n",
      "September 12, 2019\n",
      "/cmp/Oyo/reviews?start=20\n",
      "September 10, 2019\n",
      "/cmp/Oyo/reviews?start=20\n",
      "September 4, 2019\n",
      "/cmp/Oyo/reviews?start=20\n",
      "September 1, 2019\n",
      "/cmp/Oyo/reviews?start=20\n",
      "August 28, 2019\n",
      "/cmp/Oyo/reviews?start=20\n",
      "August 28, 2019\n",
      "/cmp/Oyo/reviews?start=20\n",
      "this is page 20\n",
      "October 16, 2019\n",
      "/cmp/Oyo/reviews\n",
      "October 27, 2019\n",
      "/cmp/Oyo/reviews\n",
      "October 17, 2019\n",
      "/cmp/Oyo/reviews\n",
      "October 14, 2019\n",
      "/cmp/Oyo/reviews\n",
      "October 14, 2019\n",
      "/cmp/Oyo/reviews\n",
      "October 10, 2019\n",
      "/cmp/Oyo/reviews\n",
      "October 9, 2019\n",
      "/cmp/Oyo/reviews\n",
      "October 6, 2019\n",
      "/cmp/Oyo/reviews\n",
      "October 4, 2019\n",
      "/cmp/Oyo/reviews\n",
      "October 3, 2019\n",
      "/cmp/Oyo/reviews\n",
      "October 3, 2019\n",
      "/cmp/Oyo/reviews\n",
      "October 3, 2019\n",
      "/cmp/Oyo/reviews\n",
      "September 30, 2019\n",
      "/cmp/Oyo/reviews\n",
      "September 29, 2019\n",
      "/cmp/Oyo/reviews\n",
      "September 24, 2019\n",
      "/cmp/Oyo/reviews\n",
      "September 12, 2019\n",
      "/cmp/Oyo/reviews\n",
      "September 10, 2019\n",
      "/cmp/Oyo/reviews\n",
      "September 4, 2019\n",
      "/cmp/Oyo/reviews\n",
      "September 1, 2019\n",
      "/cmp/Oyo/reviews\n",
      "August 28, 2019\n",
      "/cmp/Oyo/reviews\n",
      "August 28, 2019\n",
      "/cmp/Oyo/reviews\n",
      "this is page 40\n",
      "October 16, 2019\n",
      "/cmp/Oyo/reviews?start=20\n",
      "October 27, 2019\n",
      "/cmp/Oyo/reviews?start=20\n",
      "October 17, 2019\n",
      "/cmp/Oyo/reviews?start=20\n",
      "October 14, 2019\n",
      "/cmp/Oyo/reviews?start=20\n",
      "October 14, 2019\n",
      "/cmp/Oyo/reviews?start=20\n",
      "October 10, 2019\n",
      "/cmp/Oyo/reviews?start=20\n",
      "October 9, 2019\n",
      "/cmp/Oyo/reviews?start=20\n",
      "October 6, 2019\n",
      "/cmp/Oyo/reviews?start=20\n",
      "October 4, 2019\n",
      "/cmp/Oyo/reviews?start=20\n",
      "October 3, 2019\n",
      "/cmp/Oyo/reviews?start=20\n",
      "October 3, 2019\n",
      "/cmp/Oyo/reviews?start=20\n",
      "October 3, 2019\n",
      "/cmp/Oyo/reviews?start=20\n",
      "September 30, 2019\n",
      "/cmp/Oyo/reviews?start=20\n",
      "September 29, 2019\n",
      "/cmp/Oyo/reviews?start=20\n",
      "September 24, 2019\n",
      "/cmp/Oyo/reviews?start=20\n",
      "September 12, 2019\n",
      "/cmp/Oyo/reviews?start=20\n",
      "September 10, 2019\n",
      "/cmp/Oyo/reviews?start=20\n",
      "September 4, 2019\n",
      "/cmp/Oyo/reviews?start=20\n",
      "September 1, 2019\n",
      "/cmp/Oyo/reviews?start=20\n",
      "August 28, 2019\n",
      "/cmp/Oyo/reviews?start=20\n",
      "August 28, 2019\n",
      "/cmp/Oyo/reviews?start=20\n",
      "this is page 60\n",
      "October 16, 2019\n",
      "/cmp/Oyo/reviews?start=40\n",
      "October 27, 2019\n",
      "/cmp/Oyo/reviews?start=40\n",
      "October 17, 2019\n",
      "/cmp/Oyo/reviews?start=40\n",
      "October 14, 2019\n",
      "/cmp/Oyo/reviews?start=40\n",
      "October 14, 2019\n",
      "/cmp/Oyo/reviews?start=40\n",
      "October 10, 2019\n",
      "/cmp/Oyo/reviews?start=40\n",
      "October 9, 2019\n",
      "/cmp/Oyo/reviews?start=40\n",
      "October 6, 2019\n",
      "/cmp/Oyo/reviews?start=40\n",
      "October 4, 2019\n",
      "/cmp/Oyo/reviews?start=40\n",
      "October 3, 2019\n",
      "/cmp/Oyo/reviews?start=40\n",
      "October 3, 2019\n",
      "/cmp/Oyo/reviews?start=40\n",
      "October 3, 2019\n",
      "/cmp/Oyo/reviews?start=40\n",
      "September 30, 2019\n",
      "/cmp/Oyo/reviews?start=40\n",
      "September 29, 2019\n",
      "/cmp/Oyo/reviews?start=40\n",
      "September 24, 2019\n",
      "/cmp/Oyo/reviews?start=40\n",
      "September 12, 2019\n",
      "/cmp/Oyo/reviews?start=40\n",
      "September 10, 2019\n",
      "/cmp/Oyo/reviews?start=40\n",
      "September 4, 2019\n",
      "/cmp/Oyo/reviews?start=40\n",
      "September 1, 2019\n",
      "/cmp/Oyo/reviews?start=40\n",
      "August 28, 2019\n",
      "/cmp/Oyo/reviews?start=40\n",
      "August 28, 2019\n",
      "/cmp/Oyo/reviews?start=40\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "\n",
    "f = open('indeedreviews.txt','w+')\n",
    "page_num= 20\n",
    "def make_soup(url):\n",
    "    thepage = urllib.request.urlopen(url)\n",
    "    soupdata = BeautifulSoup(thepage,\"html.parser\")\n",
    "    return soupdata\n",
    "\n",
    "def scrape_now():\n",
    "\n",
    "    page_num=0\n",
    "    user_input = input(\"Enter a number in multiples of 20 : \") \n",
    "    page_limit = \"start=\"+user_input\n",
    "\n",
    "    date_soup= make_soup(\"https://www.indeed.co.in/cmp/Oyo/reviews?start=\"+str(page_num))\n",
    "    date_sort = date_soup.find(attrs={\"class\":\"cmp-review-date-created\"})\n",
    "    #date_sort = date_soup.find_all('span', {\"class\": \"cmp-review-date-created\"})\n",
    "\n",
    "\n",
    "\n",
    "    #while \"2019\" in date_sort.text:\n",
    "    while (True):    \n",
    "    \n",
    "        soup = make_soup(\"https://www.indeed.co.in/cmp/Oyo/reviews?start=\"+str(page_num))\n",
    "    # for the next button\n",
    "        nxt_btn = soup.find(attrs={\"class\":\"cmp-Pagination-link cmp-Pagination-link--nav\"})\n",
    "        #date_soup = make_soup(\"https://www.indeed.co.in/cmp/Oyo/reviews?start=\"+str(page_num))\n",
    "        #date_sort = date_soup.find(attrs={\"class\":\"cmp-review-date-created\"})\n",
    "        print(\"this is page \"+ str(page_num))\n",
    "        #print(nxt_btn.get('href'))\n",
    "        for date in date_soup.find_all('span', {\"class\": \"cmp-review-date-created\"}):\n",
    "            print(date.text)\n",
    "            print(nxt_btn.get('href'))\n",
    "        \n",
    "    \n",
    "        next_button = nxt_btn.get('href')\n",
    "   \n",
    "        page_num = page_num +20\n",
    "        soup_1 = make_soup(\"https://www.indeed.co.in\"+ nxt_btn.get('href')+'/')\n",
    "    \n",
    "    \n",
    "    \n",
    "        if page_limit in next_button:\n",
    "            break\n",
    "        \n",
    "        \n",
    "        #soup = make_soup(\"https://www.indeed.co.in/cmp/Indeed/reviews?fcountry=ALL&start=40\")\n",
    "        for data in soup.find_all('span', {\"class\": \"cmp-review-text\"}):\n",
    "        \n",
    "        #print(data.text)\n",
    "            f.write(str(data.text.encode(\"utf-8\")))\n",
    "        #print(f.readline())\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "scrape_now()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is page 0\n",
      "/cmp/Samsung-Electronics-9/reviews?start=20\n",
      "this is page 20\n",
      "/cmp/Samsung-Electronics-9/reviews\n",
      "this is page 40\n",
      "/cmp/Samsung-Electronics-9/reviews?start=20\n",
      "this is page 60\n",
      "/cmp/Samsung-Electronics-9/reviews?start=40\n",
      "this is page 80\n",
      "/cmp/Samsung-Electronics-9/reviews?start=60\n",
      "this is page 100\n",
      "/cmp/Samsung-Electronics-9/reviews?start=80\n",
      "this is page 120\n",
      "/cmp/Samsung-Electronics-9/reviews?start=100\n",
      "this is page 140\n",
      "/cmp/Samsung-Electronics-9/reviews?start=120\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import sys\n",
    "import xlrd\n",
    "import openpyxl\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandas import ExcelWriter\n",
    "from xlrd import open_workbook,cellname\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "\n",
    "\n",
    "f = open('negative_filler.txt','w+')\n",
    "page_num= 0\n",
    "def make_soup(url):\n",
    "    thepage = urllib.request.urlopen(url)\n",
    "    soupdata = BeautifulSoup(thepage,\"html.parser\")\n",
    "    return soupdata\n",
    "while page_num < 160:\n",
    "    \n",
    "    soup = make_soup(\"https://www.indeed.co.in/cmp/Samsung-Electronics-9/reviews?start=\"+str(page_num)+\"&sort=rating_asc\")\n",
    "    # for the next button\n",
    "    nxt_btn = soup.find(attrs={\"class\":\"cmp-Pagination-link cmp-Pagination-link--nav\"})\n",
    "    print(\"this is page \"+ str(page_num))\n",
    "    print(nxt_btn.get('href'))\n",
    "    \n",
    "   \n",
    "    page_num = page_num +20\n",
    "    soup_1 = make_soup(\"https://www.indeed.co.in\"+ nxt_btn.get('href')+'/')\n",
    "    \n",
    "   #<div class=\"mt-md common__EiReviewTextStyles__allowLineBreaks\"><p class=\"strong\">Pros</p><p>Good work environment\n",
    "\n",
    "    #soup = make_soup(\"https://www.indeed.co.in/cmp/Indeed/reviews?fcountry=ALL&start=40\")\n",
    "    for data in soup.find_all('span', {\"class\": \"cmp-review-text\"}):\n",
    "        \n",
    "        #print(data.text)\n",
    "        f.write(str(data.text.encode(\"utf-8\")))\n",
    "    #print(f.readline())\n",
    "    \n",
    "    \n",
    "f.close()\n",
    "\n",
    "\n",
    "# now we plan to go ahead with the sentiment analysis process.\n",
    "\n",
    "\n",
    "        \n",
    "   \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 403: Forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-da16494cf341>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[0mpage_num\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_soup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"https://www.glassdoor.co.in/Reviews/Apple-Reviews-E1138_P\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m \u001b[1;34m\".htm\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[1;31m# for the next button\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mnxt_btn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"class\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"pagination__ArrowStyle__nextArrow\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-da16494cf341>\u001b[0m in \u001b[0;36mmake_soup\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mpage_num\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmake_soup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mthepage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0msoupdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthepage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"html.parser\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msoupdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\softwares\\envs\\py3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\softwares\\envs\\py3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    530\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\softwares\\envs\\py3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    640\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             response = self.parent.error(\n\u001b[1;32m--> 642\u001b[1;33m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[0;32m    643\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\softwares\\envs\\py3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    568\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'default'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'http_error_default'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[1;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\softwares\\envs\\py3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    502\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 504\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    505\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\softwares\\envs\\py3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    648\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 650\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    651\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 403: Forbidden"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import sys\n",
    "import xlrd\n",
    "import openpyxl\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandas import ExcelWriter\n",
    "from xlrd import open_workbook,cellname\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "\n",
    "\n",
    "f = open('glassdoor.txt','w+')\n",
    "page_num= 0\n",
    "def make_soup(url):\n",
    "    thepage = urllib.request.urlopen(url)\n",
    "    soupdata = BeautifulSoup(thepage,\"html.parser\")\n",
    "    return soupdata\n",
    "while page_num < 3:\n",
    "    \n",
    "    soup = make_soup(\"https://www.glassdoor.co.in/Reviews/Apple-Reviews-E1138_P\"+str(page_num)+ \".htm\")\n",
    "    # for the next button\n",
    "    nxt_btn = soup.find(attrs={\"class\":\"pagination__ArrowStyle__nextArrow\"})\n",
    "    print(\"this is page \"+ str(page_num))\n",
    "    print(nxt_btn.get('href'))\n",
    "    \n",
    "   \n",
    "    page_num = page_num +1\n",
    "    soup_1 = make_soup(\"https://www.glassdoor.co.in\"+ nxt_btn.get('href')+'/')\n",
    "    \n",
    "   #<div class=\"mt-md common__EiReviewTextStyles__allowLineBreaks\"><p class=\"strong\">Pros</p><p>Good work environment\n",
    "\n",
    "    #soup = make_soup(\"https://www.indeed.co.in/cmp/Indeed/reviews?fcountry=ALL&start=40\")\n",
    "    for data in soup.find_all('div', {\"class\": \"mt-md common__EiReviewTextStyles__allowLineBreaks\"}):\n",
    "        \n",
    "        #print(data.text)\n",
    "        f.write(str(data.text.encode(\"utf-8\")))\n",
    "    #print(f.readline())\n",
    "    \n",
    "    \n",
    "f.close()\n",
    "\n",
    "\n",
    "# now we plan to go ahead with the sentiment analysis process.\n",
    "\n",
    "\n",
    "        \n",
    "   \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "tex =  open('indeedreviews.txt','w+')\n",
    "raw_data = tex.readlines()\n",
    "\n",
    "my_save_data = open(\"testFile.txt\", \"a\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pat = ('(?<!Dr)(?<!Esq)\\. +(?=[A-Z])')\n",
    "for t in raw_data:\n",
    "    if \".\" in lines:\n",
    "\n",
    "        re_lines = lines.replace(\".\", \".\\r\\n\")\n",
    "        my_save_data.write(re_lines)\n",
    "\n",
    "    else:\n",
    "        my_save_data.write(lines + \"\\n\")\n",
    "\n",
    "my_save_data.close()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
